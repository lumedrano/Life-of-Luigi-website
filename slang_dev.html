<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>I Am Luigi</title>
<link rel="stylesheet" href="style.css">
<link href="https://fonts.googleapis.com/css?family=Poppins:100,200,300,400,600,700&display=swap" rel="stylesheet">
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> 
</head>
<body>
    <section class="sub-header">
        <nav>
            <div class="nav-links" id="navLinks">  
                <i class="fa fa-close" onclick="hideMenu()"></i>
                <ul>
                    <li><a href="index.html">HOME</a></li>
                    <li><a href="slang_dev.html">S.LANG AI</a></li>
                    <li><a href="about.html">ABOUT ME</a></li>
                    <li><a href="resume.html"> RESUME</a></li>
                    <li><a href="https://github.com/lumedrano">MY GITHUB</a></li>
                    <li><a href="contact.html">CONTACT</a></li>
                </ul>
            </div>
            <i class="fa fa-bars" onclick="showMenu()"></i>
        </nav>   
            <h1>S.lang AI Development</h1>
    </section>

<!---- About SLANG AI content------>
<section>
    <div class="row reveal">
        <div class="about-col">
            <h1 style="text-align: center;">About S.LANG AI</h1>
            <p>S.lang AI is a cutting-edge, next-generation AI/ML software solution designed to break barriers in the world of sign language translation. Unlike many existing solutions available today, S.lang AI is a trailblazer in its field, providing a bidirectional translation experience, taking sign language communication to a whole new level. </p>
            <p>Traditionally, sign language translation tools have been limited to a unidirectional approach, converting sign gestures into written or spoken text. While these solutions have undoubtedly been transformative for the deaf and hard-of-hearing communities, there has been a crucial missing link in the process - enabling the translation of spoken or written text back into sign language. S.lang AI bridges this gap with unparalleled proficiency.</p>
        </div>
        <div class="about-col">
            <img class="slang_dev_logo" src="images/slangaibanner.png">
        </div>
    </div>
</section>


<!---- What to expect content---->
<!---- TODO: work on getting bulletpoints structured on left side of page & get animation working--------->
<section>
    <div class="row reveal">
        <div class="about_col">
            <h1 style="text-align: center;">Here is What to Expect</h1>
            <p class="slide-in-left">- <strong>Translation:</strong> S.lang AI boasts an unprecedented two-way translation capability, allowing users to easily switch between spoken or written language and sign language with seamless accuracy. This groundbreaking feature opens up new opportunities for inclusive communication, facilitating natural conversations between individuals who use different languages.</p>
            <p class="slide-in-left">- <strong>AI/ML Algorithms:</strong> Our software leverages the power of state-of-the-art Artificial Intelligence and Machine Learning algorithms. Through extensive training on vast sign language datasets, S.lang AI can accurately interpret and generate sign gestures with remarkable precision, ensuring a high level of fluency and authenticity in both directions.</p>
            <p class="slide-in-left">- <strong>User Interface:</strong> We believe that accessibility is paramount. S.lang AI comes with an intuitive, user-friendly interface that is easy to navigate for both seasoned sign language users and newcomers alike. The interface supports various input methods, including text, speech, and even image recognition for sign gestures, making it a versatile tool suitable for various scenarios.</p>
            <p class="slide-in-left">- <strong>Learning:</strong> As an AI/ML-based solution, S.lang AI continues to learn and improve over time. Regular updates and improvements based on user feedback and advancements in the field of AI and sign language research ensure that the software remains at the forefront of technology.</p>
        </div>
        <div class="about-col">
            <video controls autoplay loop muted style="height: 420px; width: 500px;"><source src="images/demo-video.mov"></video>
        </div>
    </div>
</section>


<section>
    <div class="row reveal">
        <div>
            <h1 style="text-align: center;">How does it Work?</h1>
            <p>- <strong>Captures Video Frames:</strong> The software begins by capturing real-time video frames from an input source, such as a webcam or a pre-recorded video. OpenCV, a popular open-source computer vision library, provides the necessary tools to access video streams and individual frames.</p>
            <p>- <strong>Hand Detection and Tracking with MediaPipe:</strong> MediaPipe, developed by Google, is a powerful framework for building multimodal applied machine learning pipelines. This software utilizes MediaPipe's Hand Tracking module to detect and track hands within each video frame. This process involves identifying key landmarks on the hand, such as fingertips, knuckles, and wrist, using a machine learning-based hand pose estimation model. The model is capable of robustly tracking hands even under various orientations, lighting conditions, and occlusions.</p>
            <p>- <strong>Hand Gesture Classification:</strong> Once MediaPipe successfully tracks the hand landmarks, the software extracts relevant hand features and encodes them into a suitable representation for classification. These features may include the spatial positions of fingers, angles between joints, or any other relevant information that characterizes different hand gestures.</p>
            <p>- <strong>Translation and Output:</strong> Once the classifier determines the most probable hand gesture from the extracted features, S.lang translates it into the corresponding letter or symbol in the chosen sign language. The output can be displayed on the screen or audibly communicated through text-to-speech synthesis, depending on the intended use of the application.</p>
            <p>- <strong>Real-Time Performace</strong> One of the key strengths of your software is its real-time performance. By leveraging efficient algorithms and optimizations provided by OpenCV and MediaPipe, this application can process video frames rapidly, providing instantaneous hand gesture recognition and translation.</p>
        </div>
    </div>
    <div class="row_portfolio_photos">
        <div class="course-col">
            <h3 style="text-align: left;">Training/Testing Accuracy</h3>
            <img src="images/slangai_accuracy2.png" class="dev_photo" onclick="toggleExpand(this)">
            <p>When Training and Testing, the results for the first iteration were promising. The training portion came back at <strong>100%</strong> while the testing came back at about <strong>99.22%</strong> for a near perfectly trained model without overfitting.</p>
        </div>
        <div class="course-col">
            <h3 style="text-align: left;">Landmark Drawing</h3>
            <img src="images/screenshot_letter(A).jpg" class="dev_photo" onclick="toggleExpand(this)">
            <p>When running the main script, landmarks are made to only outline each finger of the user. This allows S.lang to recognize different gesetures without focusing on other figures within the image.</p>
        </div>
        <div class="course-col">
            <h3 style="text-align: left;">Rapid Classification</h3>
            <img src="images/screenshot_letter(I).jpg" class="dev_photo" onclick="toggleExpand(this)">
            <p>Upon further trials, it could be concluded that gensture to text translation was made rapidly with minimal latency issues.</p>
        </div>
        <!-- next three use same class for sizing-->
        <div class="course-col">
            <h3 style="text-align: left;">Motion Tracking and Classification</h3>
            <video controls autoplay loop muted class="dev_video"><source src="images/motion-tracking-mp.mov" onclick="toggleExpand(this)"></video>
            <p>As you can see from this video, we were also able to account for gestures that may require movement such as 'J' or 'Z' letters. Next, we will want to continually train the model on more gestures and engineer it to translate from text to gesture using DALLE models for text to image generation.</p>
        </div>
    </div>
</section>



<!------- Whats Next content -------->
<section style="text-align: center; padding-left: 5%;">
    <div class="row reveal">
        <div>
        <h1>Whats Next?</h1> 
        <p>What we expect to accomplish in the following months is a full scale model that can work via desktop or phone in a bidirectional translation method.</p>
        <p>To achieve dual translation, we want to incorporate AI image generation that can create creative images based on prompt or direct text as specified by the user.</p>
        <p>In terms of security, we will follow strict protocols in order to achieve a safe/appropriate speaking environment for users along with the addition of multi-language selection</p>    
        </div>  
    </div>
</section>



<!-------- footer ---------->

<section class="footer">
    <h4>Media and More</h4>
    <p>Feel free to connect with me on LinkedIn, Instagram, or take a look at some of my projects on Github!</p>
    <div class="icons">
        <a href="https://www.instagram.com/luigi_med03/"><i class="fa fa-instagram"></i></a>
        <a href="www.linkedin.com/in/luigi-medrano"><i class="fa fa-linkedin"></i></a>
        <a href="https://github.com/lumedrano"><i class="fa fa-github"></i></a>
  
        
    </div>
</section>       
  
  
<!----JavaScript for toggle menu---->
<script>
    //for menu
    var navLinks = document.getElementById("navLinks");

    function showMenu() {
        navLinks.style.right = "0";
    }

    function hideMenu() {
        navLinks.style.right = "-200px";
    }

    //for slidein/fadein effect of bullet points
    function reveal() {
  var reveals = document.querySelectorAll(".reveal");

  for (var i = 0; i < reveals.length; i++) {
    var windowHeight = window.innerHeight;
    var elementTop = reveals[i].getBoundingClientRect().top;
    var elementVisible = 150;

    if (elementTop < windowHeight - elementVisible) {
      reveals[i].classList.add("active");
    } else {
      reveals[i].classList.remove("active");
    }
  }
}

window.addEventListener("scroll", reveal);
reveal();

//this expands the photos when clicked on 
function toggleExpand(element) {
    element.classList.toggle('expanded');
}
</script> 
    
</body>
</html>    